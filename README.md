# âš™ï¸ Hyperparameter Optimization using Optuna, GridSearchCV & RandomizedSearchCV

## ğŸ“Œ Project Overview

This project demonstrates advanced hyperparameter tuning techniques to improve machine learning model performance.
An XGBoost classifier is trained on a classification dataset and optimized using three different tuning methods:

* Optuna (Bayesian Optimization)
* GridSearchCV
* RandomizedSearchCV

The goal is to compare optimization strategies and evaluate how hyperparameter tuning enhances model accuracy and ROC-AUC performance.

---

## ğŸš€ Key Features

* Data preprocessing and feature scaling
* Baseline XGBoost model training
* Hyperparameter tuning using **Optuna**
* Traditional tuning using **GridSearchCV**
* Efficient tuning using **RandomizedSearchCV**
* Performance comparison of all approaches
* Evaluation using Accuracy and ROC-AUC

---

## ğŸ› ï¸ Technologies Used

* Python
* Scikit-learn
* XGBoost
* Optuna
* Pandas & NumPy

---

## ğŸ“Š Workflow

1. Load and preprocess dataset
2. Train baseline XGBoost model
3. Evaluate baseline performance
4. Optimize hyperparameters using Optuna
5. Train and evaluate Optuna-tuned model
6. Perform GridSearchCV tuning
7. Perform RandomizedSearchCV tuning
8. Compare all results

---

## ğŸ“ˆ Evaluation Metrics

* Accuracy Score
* ROC-AUC Score
* Cross-validation performance

These metrics help determine the effectiveness of different hyperparameter optimization techniques.

---

## ğŸ“‚ Project Structure

```
Hyperparameter-Optimization-Optuna-vs-GridSearch
â”‚â”€â”€ hyperparameter_optimization.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the script

```
python hyperparameter_optimization.py
```

---

## ğŸ§  Learning Outcomes

* Understanding importance of hyperparameter tuning
* Using Optuna for advanced optimization
* Comparing Grid Search vs Random Search vs Bayesian optimization
* Improving model performance using tuning techniques
* Building a structured ML optimization workflow

---

## ğŸ”® Future Improvements

* Add visualization of Optuna trials
* Use cross-validation inside Optuna objective
* Apply tuning on real-world datasets
* Deploy optimized model as API or web app

---

## ğŸ‘¨â€ğŸ’» Author

**Prateek Manjunath**</br>
Machine Learning & Data Science Enthusiast
